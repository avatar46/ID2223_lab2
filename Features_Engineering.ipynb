{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering Pipeline"
      ],
      "metadata": {
        "id": "3vR20QdP44l4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ee227d-60c5-44bf-b04d-c2092f997454",
        "outputId": "ae24b446-f6c8-4441-bdc1-2cc89c3e6711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 14.2 kB/88.\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:13 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,307 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [30.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,348 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,519 kB]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Get:21 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 Packages [12.5 kB]\n",
            "Fetched 6,581 kB in 6s (1,039 kB/s)\n",
            "Reading package lists... Done\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "17 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libaom0 libavcodec58 libavdevice58 libavfilter7 libavformat58 libavresample4\n",
            "  libavutil56 libcodec2-0.7 liblilv-0-0 libmysofa1 libpocketsphinx3\n",
            "  libpostproc55 librabbitmq4 libserd-0-0 libsord-0-0 libsphinxbase3\n",
            "  libsratom-0-0 libsrt1-gnutls libswresample3 libswscale5 libvidstab1.1\n",
            "  libx264-155 libx265-192 libzimg2\n",
            "Suggested packages:\n",
            "  ffmpeg-doc serdi sordi\n",
            "Recommended packages:\n",
            "  pocketsphinx-hmm-en-hub4wsj | pocketsphinx-hmm-zh-tdt\n",
            "  | pocketsphinx-hmm-en-tidigits pocketsphinx-lm-en-hub4\n",
            "  | pocketsphinx-lm-zh-hans-gigatdt | pocketsphinx-lm-zh-hant-gigatdt\n",
            "The following NEW packages will be installed:\n",
            "  libaom0 libavcodec58 libavdevice58 libavfilter7 libavformat58 libavresample4\n",
            "  libavutil56 libcodec2-0.7 liblilv-0-0 libmysofa1 libpocketsphinx3\n",
            "  libpostproc55 librabbitmq4 libserd-0-0 libsord-0-0 libsphinxbase3\n",
            "  libsratom-0-0 libsrt1-gnutls libswresample3 libswscale5 libvidstab1.1\n",
            "  libx264-155 libx265-192 libzimg2\n",
            "The following packages will be upgraded:\n",
            "  ffmpeg\n",
            "1 upgraded, 24 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 13.4 MB of archives.\n",
            "After this operation, 49.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcodec2-0.7 amd64 0.7-1 [202 kB]\n",
            "Get:2 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libaom0 amd64 1.0.0.errata1-3~18.04.york0 [1,165 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 librabbitmq4 amd64 0.8.0-1ubuntu0.18.04.2 [33.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserd-0-0 amd64 0.28.0~dfsg0-1 [37.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsord-0-0 amd64 0.16.0~dfsg0-1 [20.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsratom-0-0 amd64 0.6.0~dfsg0-1 [15.8 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblilv-0-0 amd64 0.24.2~dfsg0-1 [38.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsphinxbase3 amd64 0.8+5prealpha+1-1 [118 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpocketsphinx3 amd64 0.8.0+real5prealpha-1ubuntu2 [122 kB]\n",
            "Get:10 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavutil56 amd64 7:4.3.2-0york0~18.04 [295 kB]\n",
            "Get:11 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libswresample3 amd64 7:4.3.2-0york0~18.04 [70.0 kB]\n",
            "Get:12 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libx264-155 amd64 2:0.155.2917+git0a84d98-2~18.04.york0 [529 kB]\n",
            "Get:13 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libx265-192 amd64 3.4-0york0~18.04 [1,086 kB]\n",
            "Get:14 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavcodec58 amd64 7:4.3.2-0york0~18.04 [4,952 kB]\n",
            "Get:15 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libsrt1-gnutls amd64 1.4.1-5~18.04.york0 [235 kB]\n",
            "Get:16 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavformat58 amd64 7:4.3.2-0york0~18.04 [1,043 kB]\n",
            "Get:17 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libmysofa1 amd64 1.0~dfsg0-2~18.04.york0 [39.3 kB]\n",
            "Get:18 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libpostproc55 amd64 7:4.3.2-0york0~18.04 [65.0 kB]\n",
            "Get:19 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libswscale5 amd64 7:4.3.2-0york0~18.04 [171 kB]\n",
            "Get:20 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libvidstab1.1 amd64 1.1.0-2~18.04.york1 [36.6 kB]\n",
            "Get:21 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libzimg2 amd64 3.0.1-0york0~18.04 [183 kB]\n",
            "Get:22 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavfilter7 amd64 7:4.3.2-0york0~18.04 [1,254 kB]\n",
            "Get:23 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavdevice58 amd64 7:4.3.2-0york0~18.04 [90.0 kB]\n",
            "Get:24 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavresample4 amd64 7:4.3.2-0york0~18.04 [67.3 kB]\n",
            "Get:25 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 ffmpeg amd64 7:4.3.2-0york0~18.04 [1,556 kB]\n",
            "Fetched 13.4 MB in 11s (1,212 kB/s)\n",
            "Selecting previously unselected package libaom0:amd64.\n",
            "(Reading database ... 124015 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libaom0_1.0.0.errata1-3~18.04.york0_amd64.deb ...\n",
            "Unpacking libaom0:amd64 (1.0.0.errata1-3~18.04.york0) ...\n",
            "Selecting previously unselected package libavutil56:amd64.\n",
            "Preparing to unpack .../01-libavutil56_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavutil56:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libcodec2-0.7:amd64.\n",
            "Preparing to unpack .../02-libcodec2-0.7_0.7-1_amd64.deb ...\n",
            "Unpacking libcodec2-0.7:amd64 (0.7-1) ...\n",
            "Selecting previously unselected package libswresample3:amd64.\n",
            "Preparing to unpack .../03-libswresample3_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libswresample3:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libx264-155:amd64.\n",
            "Preparing to unpack .../04-libx264-155_2%3a0.155.2917+git0a84d98-2~18.04.york0_amd64.deb ...\n",
            "Unpacking libx264-155:amd64 (2:0.155.2917+git0a84d98-2~18.04.york0) ...\n",
            "Selecting previously unselected package libx265-192:amd64.\n",
            "Preparing to unpack .../05-libx265-192_3.4-0york0~18.04_amd64.deb ...\n",
            "Unpacking libx265-192:amd64 (3.4-0york0~18.04) ...\n",
            "Selecting previously unselected package libavcodec58:amd64.\n",
            "Preparing to unpack .../06-libavcodec58_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavcodec58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package librabbitmq4:amd64.\n",
            "Preparing to unpack .../07-librabbitmq4_0.8.0-1ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking librabbitmq4:amd64 (0.8.0-1ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package libsrt1-gnutls:amd64.\n",
            "Preparing to unpack .../08-libsrt1-gnutls_1.4.1-5~18.04.york0_amd64.deb ...\n",
            "Unpacking libsrt1-gnutls:amd64 (1.4.1-5~18.04.york0) ...\n",
            "Selecting previously unselected package libavformat58:amd64.\n",
            "Preparing to unpack .../09-libavformat58_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavformat58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libserd-0-0:amd64.\n",
            "Preparing to unpack .../10-libserd-0-0_0.28.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libserd-0-0:amd64 (0.28.0~dfsg0-1) ...\n",
            "Selecting previously unselected package libsord-0-0:amd64.\n",
            "Preparing to unpack .../11-libsord-0-0_0.16.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libsord-0-0:amd64 (0.16.0~dfsg0-1) ...\n",
            "Selecting previously unselected package libsratom-0-0:amd64.\n",
            "Preparing to unpack .../12-libsratom-0-0_0.6.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libsratom-0-0:amd64 (0.6.0~dfsg0-1) ...\n",
            "Selecting previously unselected package liblilv-0-0.\n",
            "Preparing to unpack .../13-liblilv-0-0_0.24.2~dfsg0-1_amd64.deb ...\n",
            "Unpacking liblilv-0-0 (0.24.2~dfsg0-1) ...\n",
            "Selecting previously unselected package libmysofa1:amd64.\n",
            "Preparing to unpack .../14-libmysofa1_1.0~dfsg0-2~18.04.york0_amd64.deb ...\n",
            "Unpacking libmysofa1:amd64 (1.0~dfsg0-2~18.04.york0) ...\n",
            "Selecting previously unselected package libsphinxbase3:amd64.\n",
            "Preparing to unpack .../15-libsphinxbase3_0.8+5prealpha+1-1_amd64.deb ...\n",
            "Unpacking libsphinxbase3:amd64 (0.8+5prealpha+1-1) ...\n",
            "Selecting previously unselected package libpocketsphinx3:amd64.\n",
            "Preparing to unpack .../16-libpocketsphinx3_0.8.0+real5prealpha-1ubuntu2_amd64.deb ...\n",
            "Unpacking libpocketsphinx3:amd64 (0.8.0+real5prealpha-1ubuntu2) ...\n",
            "Selecting previously unselected package libpostproc55:amd64.\n",
            "Preparing to unpack .../17-libpostproc55_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libpostproc55:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libswscale5:amd64.\n",
            "Preparing to unpack .../18-libswscale5_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libswscale5:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libvidstab1.1:amd64.\n",
            "Preparing to unpack .../19-libvidstab1.1_1.1.0-2~18.04.york1_amd64.deb ...\n",
            "Unpacking libvidstab1.1:amd64 (1.1.0-2~18.04.york1) ...\n",
            "Selecting previously unselected package libzimg2.\n",
            "Preparing to unpack .../20-libzimg2_3.0.1-0york0~18.04_amd64.deb ...\n",
            "Unpacking libzimg2 (3.0.1-0york0~18.04) ...\n",
            "Selecting previously unselected package libavfilter7:amd64.\n",
            "Preparing to unpack .../21-libavfilter7_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavfilter7:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libavdevice58:amd64.\n",
            "Preparing to unpack .../22-libavdevice58_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavdevice58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Selecting previously unselected package libavresample4:amd64.\n",
            "Preparing to unpack .../23-libavresample4_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking libavresample4:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Preparing to unpack .../24-ffmpeg_7%3a4.3.2-0york0~18.04_amd64.deb ...\n",
            "Unpacking ffmpeg (7:4.3.2-0york0~18.04) over (7:3.4.11-0ubuntu0.1) ...\n",
            "Setting up libx264-155:amd64 (2:0.155.2917+git0a84d98-2~18.04.york0) ...\n",
            "Setting up libsphinxbase3:amd64 (0.8+5prealpha+1-1) ...\n",
            "Setting up libavutil56:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libzimg2 (3.0.1-0york0~18.04) ...\n",
            "Setting up libpocketsphinx3:amd64 (0.8.0+real5prealpha-1ubuntu2) ...\n",
            "Setting up libpostproc55:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libavresample4:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libsrt1-gnutls:amd64 (1.4.1-5~18.04.york0) ...\n",
            "Setting up libmysofa1:amd64 (1.0~dfsg0-2~18.04.york0) ...\n",
            "Setting up libvidstab1.1:amd64 (1.1.0-2~18.04.york1) ...\n",
            "Setting up libcodec2-0.7:amd64 (0.7-1) ...\n",
            "Setting up librabbitmq4:amd64 (0.8.0-1ubuntu0.18.04.2) ...\n",
            "Setting up libaom0:amd64 (1.0.0.errata1-3~18.04.york0) ...\n",
            "Setting up libserd-0-0:amd64 (0.28.0~dfsg0-1) ...\n",
            "Setting up libx265-192:amd64 (3.4-0york0~18.04) ...\n",
            "Setting up libswscale5:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libswresample3:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libsord-0-0:amd64 (0.16.0~dfsg0-1) ...\n",
            "Setting up libsratom-0-0:amd64 (0.6.0~dfsg0-1) ...\n",
            "Setting up libavcodec58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up liblilv-0-0 (0.24.2~dfsg0-1) ...\n",
            "Setting up libavformat58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libavfilter7:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up libavdevice58:amd64 (7:4.3.2-0york0~18.04) ...\n",
            "Setting up ffmpeg (7:4.3.2-0york0~18.04) ...\n",
            "Removing obsolete conffile /etc/ffserver.conf ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
        "outputId": "c65c41f4-fcaf-490b-d499-26345f873d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-hi88impf\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-hi88impf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (0.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.26.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2022.9.24)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.26.0.dev0-py3-none-any.whl size=5858215 sha256=7a107637f2a74f4714254e3ca7462d6b4dd77b6686922a5bf49d344698df0f71\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fn4q0i51/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.26.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 7.2 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 30.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.12.0-py3-none-any.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 48.6 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.23.0)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.23.1-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[K     |████████████████████████████████| 278 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (4.1.1)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.2-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.9 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.16.1-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.8 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.16.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.0 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.6)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=280fe7bc28d48db24362ebded36ca60e3e45e9ebc2909947a4fdd1a7f3a04b01\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=b85573ff3eeb344903541bfbfc1843f157a70db542f41d85ed7d6a0ca2a44fc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\n",
            "Successfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.4 fastapi-0.88.0 ffmpy-0.3.0 gradio-3.12.0 h11-0.12.0 httpcore-0.15.0 httpx-0.23.1 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.3 paramiko-2.12.0 pycryptodome-3.16.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.22.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hopsworks\n",
            "  Downloading hopsworks-3.0.4.tar.gz (35 kB)\n",
            "Collecting hsfs[python]<3.1.0,>=3.0.0\n",
            "  Downloading hsfs-3.0.5.tar.gz (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting hsml<3.1.0,>=3.0.0\n",
            "  Downloading hsml-3.0.3.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyhumps==1.6.1\n",
            "  Downloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from hopsworks) (2.23.0)\n",
            "Collecting furl\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.22-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting pyjks\n",
            "  Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from hopsworks) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.21.6)\n",
            "Collecting avro==1.10.2\n",
            "  Downloading avro-1.10.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.8/dist-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.4.44)\n",
            "Collecting PyMySQL[rsa]\n",
            "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting great_expectations==0.14.12\n",
            "  Downloading great_expectations-0.14.12-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe<2.1.0 in /usr/local/lib/python3.8/dist-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.0.1)\n",
            "Collecting pyhopshive[thrift]\n",
            "  Downloading PyHopsHive-0.6.4.1.dev0.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (9.0.0)\n",
            "Collecting confluent-kafka==1.8.2\n",
            "  Downloading confluent_kafka-1.8.2-cp38-cp38-manylinux2010_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 32.5 MB/s \n",
            "\u001b[?25hCollecting fastavro==1.4.11\n",
            "  Downloading fastavro-1.4.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.7.3)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.7.1)\n",
            "Collecting pyparsing<3,>=2.4\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml<0.17.18,>=0.16\n",
            "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (21.3)\n",
            "Collecting colorama>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: mistune<2.0.0,>=0.8.4 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.8.4)\n",
            "Requirement already satisfied: pytz>=2021.3 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2022.6)\n",
            "Requirement already satisfied: Click>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.1.2)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (38.0.4)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.3.3)\n",
            "Requirement already satisfied: nbformat>=5.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.1.1)\n",
            "Collecting jsonpatch>=1.22\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: altair<5,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.13.0)\n",
            "Requirement already satisfied: jinja2<3.1.0,>=2.10 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.25.11)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=3.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=3.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.7.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.10.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.0.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.6.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (6.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (6.1.12)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.8.3)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (22.1.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.11.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.16.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->hopsworks) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->hopsworks) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->hopsworks) (3.0.4)\n",
            "Collecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (555 kB)\n",
            "\u001b[K     |████████████████████████████████| 555 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.7.16)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.15.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.13.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.7.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.5.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.22\n",
            "  Downloading botocore-1.29.22-py3-none-any.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 52.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyhopshive[thrift]->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.16.0)\n",
            "Collecting thrift>=0.10.0\n",
            "  Downloading thrift-0.16.0.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting twofish\n",
            "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from pyjks->hopsworks) (0.4.8)\n",
            "Collecting pycryptodomex\n",
            "  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 38.8 MB/s \n",
            "\u001b[?25hCollecting javaobj-py3\n",
            "  Downloading javaobj_py3-0.4.3-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.8/dist-packages (from pyjks->hopsworks) (0.2.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.0.1)\n",
            "Building wheels for collected packages: hopsworks, hsfs, avro, hsml, pyhopshive, thrift, twofish\n",
            "  Building wheel for hopsworks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopsworks: filename=hopsworks-3.0.4-py3-none-any.whl size=64205 sha256=a1473e6dd6821d2611e70694cef45d1e8b08a767de81a968ca11e96222628b47\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/6c/29/cc574a7f42e873dccd45d7e7b0dbe4d55b0320925fb7969b4a\n",
            "  Building wheel for hsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsfs: filename=hsfs-3.0.5-py3-none-any.whl size=176226 sha256=f7b06009be9895620128519cbbc302459bd7595a0debabee3d912afbb73cf47f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/dd/82/9c0962e6557feae899fa816298fd24838551002324db64467f\n",
            "  Building wheel for avro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.10.2-py3-none-any.whl size=96832 sha256=8519355116c60077bd51bf69f64199f4cfeefdf8c8a110e300005ba5fdb3df00\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/b5/b3/185a0da0ecbc3e902e24d1e2fa415db0c7342d6e3633c49d30\n",
            "  Building wheel for hsml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsml: filename=hsml-3.0.3-py3-none-any.whl size=99264 sha256=0ab9b8c85179e2e830b7200fd5d24613bfb3e25605c8c4965386fe251625eeaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/37/f3/2a92d3600a59c23db43c3d2a431d35a87afc1940898b0725f3\n",
            "  Building wheel for pyhopshive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhopshive: filename=PyHopsHive-0.6.4.1.dev0-py3-none-any.whl size=48585 sha256=e69319ea7244ef5665d931d5866593e5f9f42e43a589bf63c15bf5a8e9cb4602\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/03/d4/a158cc8e71936cd368e21f6b786a9bf2a20049059dc5c75e52\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thrift: filename=thrift-0.16.0-cp38-cp38-linux_x86_64.whl size=349402 sha256=14e21d082a3f465d90e16e62ba190e7c5c0586599026fcb148b6c5381e2027d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/03/53/7220f8aeecbf2c66ef655a9cff1929299e38119502a316ae30\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twofish: filename=twofish-0.3.0-cp38-cp38-linux_x86_64.whl size=27137 sha256=e5bb011ab9076b909367a4c70b60b192937bac974c6f6126ca5cab308a9b062c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/d8/e3/c2de52855d46f9c708b45a0159f2cfa1c2c72ae7179ea6289b\n",
            "Successfully built hopsworks hsfs avro hsml pyhopshive thrift twofish\n",
            "Installing collected packages: jedi, jmespath, twofish, ruamel.yaml.clib, pyparsing, pycryptodomex, jsonpointer, javaobj-py3, botocore, s3transfer, ruamel.yaml, PyMySQL, pyjks, orderedmultidict, jsonpatch, colorama, thrift, pyhumps, pyhopshive, mock, great-expectations, furl, boto3, avro, hsfs, fastavro, confluent-kafka, hsml, hopsworks\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "Successfully installed PyMySQL-1.0.2 avro-1.10.2 boto3-1.26.22 botocore-1.29.22 colorama-0.4.6 confluent-kafka-1.8.2 fastavro-1.4.11 furl-2.1.3 great-expectations-0.14.12 hopsworks-3.0.4 hsfs-3.0.5 hsml-3.0.3 javaobj-py3-0.4.3 jedi-0.18.2 jmespath-1.0.1 jsonpatch-1.32 jsonpointer-2.3 mock-4.0.3 orderedmultidict-1.0.1 pycryptodomex-3.16.0 pyhopshive-0.6.4.1.dev0 pyhumps-1.6.1 pyjks-20.0.0 pyparsing-2.4.7 ruamel.yaml-0.17.17 ruamel.yaml.clib-0.2.7 s3transfer-0.6.0 thrift-0.16.0 twofish-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install hopsworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "4a11b98356304a5783e1be056094d5dd",
            "c7954fdf9b674e6d946c64fc32023f6a",
            "c622cb67073848fa90ef19858bc7348c",
            "4f1472c059384584abb63d41f0de7583",
            "28b086fa5aa841d6bcaa59a1dd345087",
            "ee5bed8c90e447fa921416e9351ea2e5",
            "f954e9f507844eafb563ded266098ee6",
            "c8be7100c8784d7999774c34dfec703a",
            "381547449ec84c1593512df0497bdc46",
            "84570b670eb4432e8c732af1f568f158",
            "d4525320b375465ea43e12593d873715",
            "5b4c7aa95e1c4b84bd80aa7d2e5eae56",
            "94f174db2192492da5895d0bde15c1e4",
            "ae94b563fee0436d992dee15bffe9a31",
            "bffceb9ca63c4e4a9700dd4562da4958",
            "90e54ebfa336409a8d3dccc0a25715ea",
            "b614701e096b4509b1e64820dd6064eb"
          ]
        },
        "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
        "outputId": "1765eed8-364e-4db5-d17c-c3091bff8604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XAf_9LpaFYK",
        "outputId": "fcccbb63-475c-4995-99cd-d3ae4cab4ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy your Api Key (first register/login): https://c.app.hopsworks.ai/account/api/generated\n",
            "\n",
            "Paste it here: ··········\n",
            "Connected. Call `.close()` to terminate connection gracefully.\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/5287\n"
          ]
        }
      ],
      "source": [
        "import hopsworks\n",
        "project = hopsworks.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
      },
      "source": [
        "Using 🤗 Datasets, downloading and preparing data is extremely simple. \n",
        "We can download and prepare the Common Voice splits in just one line of code. \n",
        "\n",
        "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
        "\n",
        "Since Hindi is very low-resource, we'll combine the `train` and `validation` \n",
        "splits to give approximately 8 hours of training data. We'll use the 4 hours \n",
        "of `test` data as our held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eed1f24a9217434091480c5a983a10ad",
            "5fe1700ce51249678c1b66e775eb5616",
            "2cc8c989ae3143cbb899c2ef0551a3de",
            "349774309715491881b6ccce09329c18",
            "d07c848113e446b2bde344b9dae92361",
            "333270d139664920aedbbd0a69756dcc",
            "4436a5d97d9f499db5c4c3d5412de763",
            "38a41455dfb24e1c8faae2f27802d2f3",
            "c7619000f85842d580551f129b05b5cb",
            "0a1042c0dda04cb0b5af8904b2393d3f",
            "b90003da96ce4c629ef1b32678289071",
            "10752e6f9a5348ee83ec84715b1ec21f",
            "3e741627241e487fa4a7a1e2779b42f9",
            "7bc98dbd63f74910bbc144c806649487",
            "1459687ffd2949109ba1192177218429",
            "a7159e1fd9bf4d78b21f7d5674013e0b",
            "b9f17a8c47af4c0f8bad9b39a3ec3aac",
            "604826bef0ce4e07a0d1795a7ec5ec44",
            "fc44ef7ec6c74f18b3ec6230f19b868b",
            "1f72ccf5afac452699c1ac7895827aa8",
            "fd1760b7944b421faed7c1f76c53cd05",
            "f6e4e7571a324131a69a31c76c2faf04",
            "f1a8f83f4e1b4b57b812c923f597d3b1",
            "acca625aa38542d29e19d0dd16c6cc38",
            "1c2e538192034e6c8774093b169b70f2",
            "2f17028c0fbf4729a55887fe160d2c71",
            "64425a8deb1346168b588a93e668f1fc",
            "f71eae2fb8f04b8583176f5f900cdbe5",
            "a1659881af8d40eeb321310c2dbc8495",
            "36fc88702493495ea457ed769c237c60",
            "721be966e3a84af1bc585adf68a129e3",
            "43ab1990006d452e8add7bccac3ee92b",
            "dd54421b2a744cc696a4b6917e6a9718",
            "8f6c50bca40f4d0b9087db0beb4a7a56",
            "6acb69d9e68746089cef7c7dabe2de81",
            "866385ae04c44475880f97de7a2abc38",
            "01aa8a5650b74f42b08ad66acc1bfa2e",
            "f881f7a853b547f8bfa661b9243e2500",
            "94cf47275e24409c87fdbf76b43e9b53",
            "d196a9173c6c45daac9e5c845df3e2b1",
            "3d17e44132564858b1f38216c24d896f",
            "682fc12a13bc480983d6fb786d6cdd16",
            "4af2fc321dc441e4ad829e2295ddfbab",
            "02927891967d4fb6a2301d79216de742",
            "0881b1e038054bec97ab0f6113e416f4",
            "01540e7252b04247b24590f366dcff5b",
            "53a74bf47a86470dbff46c1bbf05ed1d",
            "786647b6ad94442b9639fc2740f8469d",
            "ddbfa069f41e4bd2a2af1d570760bdd7",
            "48e6dd2a09ae41f1be9a9426fd181afa",
            "2058183693da43aea5b732bdde152925",
            "dd60550a0b5a4b159c64e1739b7f45d4",
            "d24d7c4140a2467a890d63dc772fa9a1",
            "2400f34cfecf471691576ed5c12023f5",
            "7db3b13eb986467eabd1796d3dce8800",
            "d3e38298c71c4844a4fdb2cac994dc0f",
            "b0317f0c2b2643e9beec7610547c036b",
            "bd3443ad38064c42992910bcdbd07517",
            "de311c8d2075471ebbc26e533b99a3a8",
            "d0a8f291bbbb46b6a91588a43f234b33",
            "3fab1ca3d87f43a3a45dd15751f0dc0e",
            "53259be1357a45cbbd53608a84b59a6c",
            "d563d8aa512142e49e3fecb7efb77176",
            "54dab9028aba4370a4a6f5a9ac37953e",
            "f1d1861487c44624a405e723d273b029",
            "c403afc3d41a46ddaed841b7dfe05795",
            "5f609c8323324409b8080f872ec734c5",
            "b007ca8361654d879273b361e1759564",
            "910045e42191404b80137764e2ec5ded",
            "b87e9927eaf54cd1a9948fb31579542d",
            "9093a5eec726475790d25f9fc97c3043",
            "d71f0339dc914e4198479ba349a14c83",
            "221c374b4e9d42789cf6698b99de970f",
            "bda04d5dff8c423289c1e49ddcf31761",
            "fb6d214a6c63453bbf911798097984b7",
            "a0944ef392ac4644b48f12b7292b329f",
            "58252ddee68d435fac399b9b06378dc2",
            "96c4d594c7ac4c2e9ecbe7fd5d37e87e",
            "09ed629b404c466b9de5ac7c4bd556c2",
            "b32daad977e04414a0d4392c579ca747",
            "d8127a7bbf8a4e92969e9768bd620953",
            "f196863e760148c7b0befd32f954a693",
            "a0484370c6a24b0baf32cb7158ff582b",
            "960c21ad0a7c45719830b982982c8a01",
            "e254926e16ff4b0dac73c19ff14a608a",
            "aaeaedc8cb7748a6b92e18a24f850d14",
            "cd618680521340b6a33b7395c763f7f4",
            "3a30ea7f0a154a9187a7d31057f7bebe",
            "042a343d7de746348961c938deb2d60f",
            "1a7f1171974c4c0f931482c063b8f087",
            "ee3293be171d4640b8400951467250cd",
            "422617a7733a45f985dbc59e1a14375c",
            "932779e6a50a44bd9a7a5be5fba92cfb",
            "c1d16e7031054499858db5b8e4cc6752",
            "1cd6102bd8134c8aa675e62fee58946c",
            "39ce50aa357b4468bf1540da64eccbf2",
            "d93e64cf89cc4269bfad0a468d61e5c0",
            "89b8a84fb8884757a59796ead6fbbf8e",
            "5817c34c257d444398a158594e01439f",
            "67651d3f97b042febaec0a78fd3c92c9",
            "2c74b941a5e043e891d3f9c74231fa22",
            "d1dacd60e8e641beba5fde1f581dc41e",
            "cf47789a80ab47a7be9a69e77ebf363a",
            "9f1c8788a2d54071a5fa57388a292850",
            "d41bbf7791ea43fbb54aacdcd9797598",
            "daffc4bcc52349cfbbdfad1284d3c383",
            "adc6491376ef4e7c983ec9ab312f64e6",
            "48413d30c3e74a46801b4ef43e387222",
            "1f9dc756f5c94ee7bc032509fb69ee33",
            "1e29f7199bb9401eb4b7c6ec98d53409",
            "65b7ed259af143b7821af33bf7039457",
            "779ae20049fb4b0a851de1af6b7664c0",
            "2c5f212bd35548f2b7928f07f5afec7c",
            "8b69b3814c644105af07afcbc877c4be",
            "0f2e3efd939846509bbc99d717424480",
            "97159bf9747c4adb86daaec2a139fbab",
            "94b579635b584a2fb7e0d1b98857b577",
            "e2c7779626204977b95469c451c0a6b8",
            "60d95042ac734a1fabc2ce5b58976192",
            "f69c429110964880b3bd3109667833a3",
            "3846fcfb8a3542e9990f7629e49b8a08",
            "ff37472d262c4952bfb85d05bd2b08aa",
            "a4df547de773434ca4c9552b5a02b675",
            "6176404f57c540f1b4da5f427f6c0b3d",
            "f4f57b571aae40729759ea451351482a",
            "91a49d08cbc648ba989722929a51662b",
            "81ed615adb48402da2ab6db7ffe9a987",
            "1ac7d7971f034dc9b544f42ebf6ad3c7",
            "a0f6de614433448b90107929c171b787",
            "bb103ed70c8e436085844259ff881e4e",
            "8c1d4dd268364a1aa0880e56fa304f06",
            "1cbe00391be045a48f32dbeadc3f98a6",
            "50fa5a846f22451b8f44cbe42ba72ed2",
            "0b177e7575cc4b6fabb6f7e74867e546",
            "f55ad3c1a78a4fa5aaa5e60acc1126bc",
            "cbb53c021e8c49208eec39faede4ba1b",
            "cd64dbb5ee07477ba7ee5275aa44eac6",
            "ec72d5e6d6ce4178a72a96aa6bd42848",
            "f258206ce863401d8df4c9c940e19eac",
            "b35e04f8fd59426c92813abec0ca2377",
            "df3da3f709874001a4d80f50b3c1988b",
            "0a2853d60cde45afbee7cfd29cf8713a",
            "6f60ed6f34dd4bfe922877ffd97eb4db",
            "4c624070046c4eafbf5ea8783c9c6d6d",
            "50b7262485bb4b8db1f348e21886c11b",
            "d4d8f17d76494ddcb6831b95867ba885",
            "ed8e5366d30f4185bfc0eb20137c1801",
            "904e165e68ef401db3e5c81d74a211d7",
            "ceb35b6b5dd14c74961b470e50e29e1b",
            "646298f63b3a4814a843feabb49aa75a",
            "a44e9ad0de0a46b6b7f66b4b5c0fe27b",
            "799d3f3d6a434918bae7e83ed8042859",
            "b7abac12212a4fa090a04b4019a5cd1a",
            "d445e9e026434905993224739c9dfab0",
            "e21a68f7aacf440688604f040b5185f9",
            "a80bcf096df546cd97df10f358466638",
            "26c5e343f4b542b086062251bbd1db57",
            "7b40ab5cca674879a7b7b0e91544038e",
            "52c53e74c3634654842845b50c1000dd",
            "da6d75f142b44bcbb7320c1d1a03e634",
            "378ffffbbc5040dd89580bcd08b11cda",
            "4241783b048e4721acaed735ed8d29d6",
            "ef54e2540e6e461fa2e258ce41f87ba1",
            "6ba4df516c8e4ad7a120b5d9092cd4e8",
            "74f19fad2ec24a49bc4fd0bb63978520",
            "e66cfb6553fc4253bd3d08bf41ee951d",
            "5b8bace18ce64b649d2980784d10707d",
            "2b4f7419639a46a7aab0d3abf5b4466f",
            "9cc300cacb79455a8f806afb2c2ca5c4",
            "ad18fa41daec4413bf5f6c3e5ecb2314",
            "066bffc57c864a5da01e68362df22efc",
            "c9c1957af1044be59b2fc14e5b474be1",
            "68b66062283b4b77bb36f8407f9aff5e",
            "12010422acb3436aad8b400d20d3a009",
            "cd89242e61294f4faf1874da146ea621",
            "4bbd61319a5f4dc9ae420c5cd26a3886",
            "434bc79b9d0e4f13b93f4e31ce6c2119",
            "0fc6e84bb0e441728c753f13cc3fce65",
            "70c3639d876a4da2bbc54f4b5ff85264",
            "cb87f2c0afa84720862e657ec3e20dbc",
            "d79b8052fb064671a26249accde05710",
            "32381871ff7d4e22b9d0d8d5e95dfca2",
            "77825d63427c4bafbd9ddc7225fa0a2a",
            "6edee380fab047c990fb863f6a53287e",
            "f7590cab75754dc18049235083af88ec",
            "d3e1f29708f141018fe8af8f778e9c58",
            "95270511ad2e4b62a7235077fa406acf",
            "7cfb4e3283054de4bbc7c5facc50f7ff",
            "c1713868bcc34ee99cc512953de91aff",
            "4a8794d3825f4049a2002974207e2fb0",
            "9e5ba158967944e7986de66c3f0231b1",
            "290715117847464e91b419482369d18d",
            "4220e44670d4452ea21f763af76f866c",
            "5933cf094af54db4a14487f264752c0d",
            "e31d5c63a3d04def98b0a473506cb3b2",
            "30347a41cbf84832b1bfbb93a1f6ab48",
            "bc4c837215334610ba1d95d1ef367624",
            "8ae1e1af30ed48e4878af72ba00f8d22",
            "ffb5350000094f2f9d78bc0d45252b3a",
            "d55eacaba8ee44b08b6499b525b95459",
            "0a9b5244640f40f3b848b645228509cc",
            "54b4756260e24c06b3d849fa9835e142",
            "f528377ee96d46debc2e62da42657ab2",
            "ca5b9910c9fe4caa82d165010f236d1d",
            "dbbc928224c140c1a4997a9d149c351f",
            "8cb6c095a37e44458824f3bb7842c6c3",
            "38bfb982481b45a8ae7655732d1ac440",
            "b572bf4c3f624d6badacc0f60993e31b",
            "5d43555aa026449995cf3db5f2f2153b",
            "3aa4933c51d54ec5bd7a12f4da0f801e",
            "9269db25f1464ff68bc60eedccd91daf",
            "5e09a69c919e4df88f913b2e89f1f6c3",
            "ff8c61af18544542b3be793c4ae6740d",
            "743587ffd55a44fd8bd0032495362f3f",
            "60a0675d2beb415e8b130e55885880f5",
            "7e89f9ccd2e949af9766135ef38eb516",
            "bc488cf42a85438e9c71d3260082f303",
            "88bb8d5f120343da80240e5ba2b410fb",
            "ad34e98146e743ecaeea03dca08fa540",
            "868bba083b1141b482c4ebcc3c3287b6",
            "deb3ed1d41764c87b8246b3662b254c2",
            "c3288430e6c048649f3366da529312b1",
            "0b2ffe2aada64bcab7cc0264cbcd640a",
            "1f20ebe1010f4ac7aebb07b4a56b1cdb",
            "8145ba9f29394c0181f1ad9df572b104",
            "d766bc9d0181492bab75fd8390bbd966",
            "392bcfc024b24334bb48a80ecd4cd0a8",
            "9743aed6f6cb4499b8e1c126d8fa99dd",
            "8ea59683425f464185f8fcc365a044cd",
            "4ced65bea532439ab692c068f9ac011d",
            "e0781b8d78614e9083c2220cf739319f",
            "b0a7644f028f4dcaa23fbe53ecb84bdd",
            "01fdbe948e1e44cd8e3e14aabcf3d462",
            "ce9ac4948fd34c20b581085780367d7b",
            "8c3b4d6605c846399a3d6d0ebdc1716f",
            "3426f7440a0d4bf1b7f5efe312277048",
            "f08cfade59014bf0b10da63ceda452f8",
            "4904726f17274c45862388f96bbb7cd7",
            "c80613acc4874e009fa9c423a7cf6700",
            "fb74297ff8fd4e0191cbfbbaa1ccea82",
            "0bef0b19a531467a9b8d72b773ca71d5",
            "92eaeafbc003481a9d90da57b55efe43",
            "d31ddeb438e34b0eb5cd4c9497a0883c",
            "e339ce6764644dc682e00ad83b29b789",
            "98f6ad67e1124377ba4cf69d2fd0b007",
            "20d807ef9b90434287e180c9a26bdce4",
            "869899e52f744ffa8c57d563f63eb8c0",
            "4dea632b734545049495f8a281bbe9ec",
            "dec6cb705f274c26929ef979076e8899",
            "434eac217efd46fc8ae5f27ae6aaab62",
            "b7d8fb0807774e8f921eb8c10ba8349e",
            "d3c398b409cb41a6aab97c5d1cb7b588",
            "a8f6b4a54ae84a579c6aebe198b03bad",
            "fad23899cbdf4a358b93c21bb8886893",
            "256306fec0454925945f724f883577a7",
            "df253ab8599143f3ab439b8a4590040b",
            "133e1dfd0acf4c8f8e7b4f1dba4bb977",
            "206b2ee659a54f90b60cddee0bfe6c53",
            "dbe22dff81474dca9bf66f1936d78a87",
            "2e05d917967c4318a736ae3e2d1f5bd3",
            "bba2926967844e55bbf0d3804af89de1",
            "85d834aab4d64be68446a022c72ee01e",
            "e43648434eb1472f865c1268ee87b653",
            "991cff9775dd4b5191fe1e36bce09be8",
            "a3859f4eb2bc4dd4a7e542a12e2da992",
            "f92e6a06a5d54873b5790ac2df253b5a",
            "b5d38dd5c5344d1e9594efa4bf4ea861",
            "6bac7d75cc4c4f2ea882da1ff2d7fc3e",
            "d94e658b31a345c987293251bfe1dea1",
            "4c608d64f731442d8b3bcbefa7682350",
            "2f3df4f0a8d34d50bff5ac35170fe531",
            "1406f2560ff544cf9a3048df76034efe",
            "3d3b503e4d164e808f5520b216be5451",
            "775e9922cd6a476a8730122f14fb1151",
            "f4df7a21543746978bdd66682ca4f473",
            "a37dafdc5a414c119c55aaf30c37e047",
            "7fbe1d8e95344e0cb012ee42a9acd4dd",
            "9bd59570235a471394ba92712bda8596",
            "7a92e12539b746b1bf1a3ea67b2b8c9c",
            "d9c408ae10494b4ca7bcec422587330d",
            "1f1a5b0721fe4e61b403a80dd9a2cbd5",
            "e351dce92be745e494d5200ab1ea3f97",
            "5464a54128c74913b0ab0aa38e4aed05",
            "9e8394118e944876ace887d52b23cac4",
            "1a7cdeccb4aa472fb6c401003c8d6f09",
            "d8db9097d099440da059f942e42a1eae",
            "b5bad6d521634dcc888d7635fa6f59d4",
            "3da318eea756440f8aec8f171b49b4cd",
            "0418225c8b284c8b9876872de89310aa",
            "82724a9c94c24b60bc8773e83b7b32a7",
            "372e531ea2dc4c9e9b3e27c993f52138",
            "147843d35c934f409dc37ce223681ac0",
            "ebd454ac4cdb47b39290c6ca67ed1338",
            "2d41993d20784779a881851fbdb64972",
            "a53e033946cd42b1ba3b865838a4be14",
            "f5ed72214f8b41de98dbee1f77002458",
            "43768761352645ee92708f06627b5ad9",
            "84ae2b05cba44bffbd6e6622fd340f74",
            "253ed2a93d114b9cbf4fea22de2c28d6",
            "bcc49a8bee284a4aaee9fed79b3ddc6a",
            "1dd19b13028e447381f7ff8c6f9b142c",
            "8a7589cd81f54a7bafb290e30ca3bf7d",
            "5ec6d76861554ae6a05324b8afd3959b",
            "89f334c267a84c008ecceefd66030963",
            "361045549a29409c97e38a4e5f61492b",
            "d670c25514f74c3089fee51de0c619e8",
            "1af948570c094bbbb371aaee453e082d",
            "a8b188c2a88640d6a4ee428c38495f72",
            "0d50522da78945f19d53767b4a2e9111",
            "adf171d126a24bc99c8f7f1c72177636",
            "ce3bd0dba92f4084b842e849cbc3fee7",
            "a238ab452a1f493eb368c33f87c79175",
            "4689b0c926fe44b1860b9da280ad13b3",
            "75974877df7d4207903d5085025994ea",
            "22af77b1030b422a83d844d1767f00da",
            "d2660e3be3e9408e93d98441925158cc",
            "d3ba1c0c46d145e985cdeda81d52845f",
            "9639a7cfa4ee4b58b7c0760fd5183c2e",
            "5207f24087b94422ad258b9b9f9f62a3",
            "c25b3873862b46c2bd0a30bf2aad0b96",
            "3aa0cf7543ab4348a1b8c9d98665bc48",
            "1f189b786d044e3fb1761440740e53d7",
            "27689e8fe2194bca9c89dc49016624ed",
            "b32b1d109b6040fd9415412d21cde37e",
            "92f6d54777c346c1a8c74f761a7be178",
            "10534b3479a6422f835f2e6f2842b6e6",
            "805e92d90d624437b1d66a567bd0cc22",
            "724c6e72038940968439a6d515e92390",
            "91490962024345018989a63f50ef6775",
            "45a33ebc57724f359b7b0bfeeae93798",
            "30652c26da1a430cbde7174a3fee668e",
            "e8406c0751fc4278b88ac97503a9f548",
            "5862a0d484db458e964056f0911025b9",
            "8dd116e693924a06b6c5c34c72282500",
            "c7b13c18f1f34880ad36ae23f6554785",
            "81e0794e088e4cfe8254fdc27ce54f1f",
            "fbaf4b9feda64bc98285b7c94b0392bb",
            "7b0f2a86f4d34de4ae8fda605dbcc968",
            "46a0ce7fa6a44020a0b7c4d800e5b232",
            "3a4cca7a8f9f4b5db785c03b59127cf9",
            "a2e21fe9f0d947fd98903ac8b7ed2406",
            "99eda384e46642759f4f416ac03486a8",
            "7146611319f64e5e9e525ee15b1922b5",
            "2afb3e2c650f4cba9980a603426ac8ce",
            "ff355d2c9fd04cce9f1624ad27789f09",
            "ed4252d462184b8ba312792ed77c6909",
            "dc9e24645e064185aef5c162411be15e",
            "f8c5819342ca4784b84bbdc5ba3e69f1",
            "52dd5b5af8294b0ca1e483bed4968ef7",
            "d8a4cdb04a17421e9249856b2e778901",
            "539422b26613443eb686728232aca911",
            "3f0757092f0443678066137141e2320d"
          ]
        },
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
        "outputId": "1116a5ae-b688-4715-a7a1-1d401e378b58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eed1f24a9217434091480c5a983a10ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10752e6f9a5348ee83ec84715b1ec21f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a8f83f4e1b4b57b812c923f597d3b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/60.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f6c50bca40f4d0b9087db0beb4a7a56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset common_voice_11_0/sv-SE to /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/sv-SE/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0881b1e038054bec97ab0f6113e416f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3e38298c71c4844a4fdb2cac994dc0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/197M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f609c8323324409b8080f872ec734c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/139M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c4d594c7ac4c2e9ecbe7fd5d37e87e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/152M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "042a343d7de746348961c938deb2d60f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/153M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67651d3f97b042febaec0a78fd3c92c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/42.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65b7ed259af143b7821af33bf7039457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #4:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff37472d262c4952bfb85d05bd2b08aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50fa5a846f22451b8f44cbe42ba72ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c624070046c4eafbf5ea8783c9c6d6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e21a68f7aacf440688604f040b5185f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #3:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e66cfb6553fc4253bd3d08bf41ee951d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "434bc79b9d0e4f13b93f4e31ce6c2119"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cfb4e3283054de4bbc7c5facc50f7ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffb5350000094f2f9d78bc0d45252b3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aa4933c51d54ec5bd7a12f4da0f801e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "deb3ed1d41764c87b8246b3662b254c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/312k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0a7644f028f4dcaa23fbe53ecb84bdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d31ddeb438e34b0eb5cd4c9497a0883c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fad23899cbdf4a358b93c21bb8886893"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #3:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3859f4eb2bc4dd4a7e542a12e2da992"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a37dafdc5a414c119c55aaf30c37e047"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files #4:   0%|          | 0/1 [00:00<?, ?obj/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5bad6d521634dcc888d7635fa6f59d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ae2b05cba44bffbd6e6622fd340f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading metadata...: 0it [00:00, ?it/s]\u001b[A\n",
            "Reading metadata...: 7308it [00:00, 57382.65it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d50522da78945f19d53767b4a2e9111"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Reading metadata...: 5052it [00:00, 55322.93it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c25b3873862b46c2bd0a30bf2aad0b96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Reading metadata...: 5069it [00:00, 69215.96it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating other split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30652c26da1a430cbde7174a3fee668e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Reading metadata...: 5699it [00:00, 80819.78it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating invalidated split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99eda384e46642759f4f416ac03486a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Reading metadata...: 1346it [00:00, 59221.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset common_voice_11_0 downloaded and prepared to /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/sv-SE/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset common_voice_11_0 (/root/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/sv-SE/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 12360\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 5069\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f"
      },
      "source": [
        "Most ASR datasets only provide input audio samples (`audio`) and the \n",
        "corresponding transcribed text (`sentence`). Common Voice contains additional \n",
        "metadata information, such as `accent` and `locale`, which we can disregard for ASR.\n",
        "Keeping the notebook as general as possible, we only consider the input audio and\n",
        "transcribed text for fine-tuning, discarding the additional metadata information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
        "outputId": "1ecef6da-b7b7-4497-9dd9-0f3bae160289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 12360\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 5069\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see \n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
        "outputId": "d104c6af-aac2-4a66-8787-529cc9fc96b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/1aaf2e61fb837413b804938857af61aeb372611d01b80b3f40d9edeb7f354244/common_voice_sv-SE_20466896.mp3', 'array': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'sampling_rate': 48000}, 'sentence': 'Du ser ut att ha gjort det här hela livet.'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
      },
      "source": [
        "Since \n",
        "our input audio is sampled at 48kHz, we need to _downsample_ it to \n",
        "16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model. \n",
        "\n",
        "We'll set the audio inputs to the correct sampling rate using dataset's \n",
        "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
        "method. This operation does not change the audio in-place, \n",
        "but rather signals to `datasets` to resample audio samples _on the fly_ the \n",
        "first time that they are loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
      },
      "source": [
        "Re-loading the first audio sample in the Common Voice dataset will resample \n",
        "it to the desired sampling rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87122d71-289a-466a-afcf-fa354b18946b",
        "outputId": "cf5305ce-f6f7-44e0-f75b-4458a89da3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/1aaf2e61fb837413b804938857af61aeb372611d01b80b3f40d9edeb7f354244/common_voice_sv-SE_20466896.mp3', 'array': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'sampling_rate': 16000}, 'sentence': 'Du ser ut att ha gjort det här hela livet.'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
      },
      "source": [
        "Now we can write a function to prepare our data ready for the model:\n",
        "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, 🤗 Datasets performs any necessary resampling operations on the fly.\n",
        "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
        "3. We encode the transcriptions to label ids through the use of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
      },
      "source": [
        "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "73c4aa48d546404fb0c696d5d495051b",
            "edc11354758848fd91917dd1341b580e",
            "6153462d324f4e6681734bb0861c584a",
            "744425b777be4836a7b53a2300ad8569",
            "8df118f8886e4344a1f179bdc96a637f",
            "50831c1bfd5e4b80a3ce62ab6ce579e8",
            "2c6f64b986834286b021a4efc4ce6b64",
            "f47528cee5c94c899acfd7bfc82b1020",
            "485cabc6126541708999cee3edb11619",
            "858d6634cc23488a82402e91d69ce135",
            "9fd715722ed74b12ae75d965c5a56f9a",
            "c3f2deb415354875b5bc725c4e2175c9",
            "8435d8a04ab44f5dad367fcab391de95",
            "1df29949ab30410697944987ec14062c",
            "6e276ee76f54454d99ce235362eee133",
            "6b5d9d3f59b4485da5cf77cd20d89bec",
            "3b537b66da554017a86ed496381419fb",
            "71b6a13fdeec4955bf324781a92afbb5",
            "1235c968eb594382b1c25ff89a9acb83",
            "2489b317a0fc4cf699e987917f032e68",
            "33010c3a56dc4662bbb05c2b364c13e0",
            "57f2dfbac49e494d94f136250469332c"
          ]
        },
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
        "outputId": "4839071f-27ee-4d63-bb5d-65873f12aaff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/12360 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73c4aa48d546404fb0c696d5d495051b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5069 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3f2deb415354875b5bc725c4e2175c9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqXelEvPgoCi",
        "outputId": "15c9156a-2acc-4699-aec4-ffe500e7734a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DatasetDict in module datasets.dataset_dict object:\n",
            "\n",
            "class DatasetDict(builtins.dict)\n",
            " |  A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DatasetDict\n",
            " |      builtins.dict\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, k) -> datasets.arrow_dataset.Dataset\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  align_labels_with_mapping(self, label2id: Dict, label_column: str) -> 'DatasetDict'\n",
            " |      Align the dataset's label ID and label name mapping to match an input :obj:`label2id` mapping.\n",
            " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
            " |      The alignment in done using the lowercase label names.\n",
            " |      \n",
            " |      Args:\n",
            " |          label2id (:obj:`dict`):\n",
            " |              The label name to ID mapping to align the dataset with.\n",
            " |          label_column (:obj:`str`):\n",
            " |              The column name of labels to align on.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
            " |      >>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
            " |      >>> # mapping to align with\n",
            " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
            " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
            " |      ```\n",
            " |  \n",
            " |  cast(self, features: datasets.features.features.Features) -> 'DatasetDict'\n",
            " |      Cast the dataset to a new set of features.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      You can also remove a column using :func:`Dataset.map` with `feature` but :func:`cast_`\n",
            " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
            " |      \n",
            " |      Args:\n",
            " |          features (:class:`datasets.Features`): New features to cast the dataset to.\n",
            " |              The name and order of the fields in the features must match the current column names.\n",
            " |              The type of the data must also be convertible from one type to the other.\n",
            " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      >>> new_features = ds[\"train\"].features.copy()\n",
            " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
            " |      >>> new_features['text'] = Value('large_string')\n",
            " |      >>> ds = ds.cast(new_features)\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
            " |       'text': Value(dtype='large_string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  cast_column(self, column: str, feature) -> 'DatasetDict'\n",
            " |      Cast column to feature for decoding.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (:obj:`str`): Column name.\n",
            " |          feature (:class:`Feature`): Target feature.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'DatasetDict'\n",
            " |      Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the tables.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`): The name of the column to cast\n",
            " |          include_nulls (`bool`, default `False`):\n",
            " |              Whether to include null values in the class labels. If True, the null values will be encoded as the `\"None\"` class label.\n",
            " |      \n",
            " |              *New in version 1.14.2*\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"boolq\")\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'answer': Value(dtype='bool', id=None),\n",
            " |       'passage': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None)}\n",
            " |      >>> ds = ds.class_encode_column(\"answer\")\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
            " |       'passage': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  cleanup_cache_files(self) -> Dict[str, int]\n",
            " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
            " |      Be careful when running this command that no other process is currently using other cache files.\n",
            " |      \n",
            " |      Return:\n",
            " |          Dict with the number of removed files for each split\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.cleanup_cache_files()\n",
            " |      {'test': 0, 'train': 0, 'validation': 0}\n",
            " |      ```\n",
            " |  \n",
            " |  filter(self, function, with_indices=False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Union[int, NoneType] = 1000, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, Union[str, NoneType]], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, fn_kwargs: Union[dict, NoneType] = None, num_proc: Union[int, NoneType] = None, desc: Union[str, NoneType] = None) -> 'DatasetDict'\n",
            " |      Apply a filter function to all the elements in the table in batches\n",
            " |      and update the table so that the dataset only includes examples according to the filter function.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Args:\n",
            " |          function (`callable`): with one of the following signature:\n",
            " |              - ``function(example: Dict[str, Any]) -> bool`` if ``with_indices=False, batched=False``\n",
            " |              - ``function(example: Dict[str, Any], indices: int) -> bool`` if ``with_indices=True, batched=False``\n",
            " |              - ``function(example: Dict[str, List]) -> List[bool]`` if ``with_indices=False, batched=True``\n",
            " |              - ``function(example: Dict[str, List], indices: List[int]) -> List[bool]`` if ``with_indices=True, batched=True``\n",
            " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
            " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
            " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
            " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
            " |          batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
            " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
            " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
            " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
            " |          fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
            " |          num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing.\n",
            " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 4265\n",
            " |          })\n",
            " |          validation: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 533\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 533\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  flatten(self, max_depth=16) -> 'DatasetDict'\n",
            " |      Flatten the Apache Arrow Table of each split (nested features are flatten).\n",
            " |      Each column with a struct type is flattened into one column per struct field.\n",
            " |      Other columns are left unchanged.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"squad\")\n",
            " |      >>> ds[\"train\"].features\n",
            " |      {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
            " |       'context': Value(dtype='string', id=None),\n",
            " |       'id': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None),\n",
            " |       'title': Value(dtype='string', id=None)}\n",
            " |      >>> ds.flatten()\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
            " |              num_rows: 87599\n",
            " |          })\n",
            " |          validation: Dataset({\n",
            " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
            " |              num_rows: 10570\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  formatted_as(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
            " |      To be used in a `with` statement. Set ``__getitem__`` return format (type and columns)\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Args:\n",
            " |          type (:obj:`str`, optional): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n",
            " |              None means ``__getitem__`` returns python objects (default)\n",
            " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
            " |              None means ``__getitem__`` returns all columns (default)\n",
            " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
            " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |  \n",
            " |  map(self, function: Union[Callable, NoneType] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Union[int, NoneType] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, Union[str, NoneType]], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, features: Union[datasets.features.features.Features, NoneType] = None, disable_nullable: bool = False, fn_kwargs: Union[dict, NoneType] = None, num_proc: Union[int, NoneType] = None, desc: Union[str, NoneType] = None) -> 'DatasetDict'\n",
            " |      Apply a function to all the elements in the table (individually or in batches)\n",
            " |      and update the table (if function does updated examples).\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Args:\n",
            " |          function (`callable`): with one of the following signature:\n",
            " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n",
            " |              - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n",
            " |              - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n",
            " |              - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n",
            " |      \n",
            " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
            " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
            " |      \n",
            " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
            " |          with_rank (:obj:`bool`, default `False`): Provide process rank to `function`. Note that in this case the\n",
            " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
            " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
            " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
            " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
            " |          batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
            " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
            " |          drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
            " |              dropped instead of being processed by the function.\n",
            " |          remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
            " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
            " |              columns with names in `remove_columns`, these columns will be kept.\n",
            " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
            " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
            " |          features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n",
            " |              instead of the automatically generated one.\n",
            " |          disable_nullable (`bool`, defaults to `False`): Disallow null values in the table.\n",
            " |          fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
            " |          num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing.\n",
            " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> def add_prefix(example):\n",
            " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
            " |      ...     return example\n",
            " |      >>> ds = ds.map(add_prefix)\n",
            " |      >>> ds[\"train\"][0:3][\"text\"]\n",
            " |      ['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
            " |       'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n",
            " |       'Review: effective but too-tepid biopic']\n",
            " |      \n",
            " |      # process a batch of examples\n",
            " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
            " |      # set number of processors\n",
            " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
            " |      ```\n",
            " |  \n",
            " |  prepare_for_task(self, task: Union[str, datasets.tasks.base.TaskTemplate], id: int = 0) -> 'DatasetDict'\n",
            " |      Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./package_reference/task_templates).\n",
            " |      \n",
            " |      Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n",
            " |      \n",
            " |      Args:\n",
            " |          task (`Union[str, TaskTemplate]`): The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n",
            " |      \n",
            " |              - `\"text-classification\"`\n",
            " |              - `\"question-answering\"`\n",
            " |      \n",
            " |              If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./package_reference/task_templates).\n",
            " |          id (`int`, defaults to 0): The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
            " |  \n",
            " |  push_to_hub(self, repo_id, private: Union[bool, NoneType] = False, token: Union[str, NoneType] = None, branch: NoneType = None, max_shard_size: Union[int, str, NoneType] = None, shard_size: Union[int, NoneType] = 'deprecated', embed_external_files: bool = True)\n",
            " |      Pushes the ``DatasetDict`` to the hub as a Parquet dataset.\n",
            " |      The ``DatasetDict`` is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
            " |      \n",
            " |      Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
            " |      \n",
            " |      The resulting Parquet files are self-contained by default: if your dataset contains :class:`Image` or :class:`Audio`\n",
            " |      data, the Parquet files will store the bytes of your images or audio files.\n",
            " |      You can disable this by setting `embed_external_files` to False.\n",
            " |      \n",
            " |      Args:\n",
            " |          repo_id (:obj:`str`):\n",
            " |              The ID of the repository to push to in the following format: ``<user>/<dataset_name>`` or\n",
            " |              ``<org>/<dataset_name>``. Also accepts ``<dataset_name>``, which will default to the namespace\n",
            " |              of the logged-in user.\n",
            " |          private (Optional :obj:`bool`):\n",
            " |              Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
            " |              a repository that already exists will not be affected by that parameter.\n",
            " |          token (Optional :obj:`str`):\n",
            " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
            " |              to the token saved locally when logging in with ``huggingface-cli login``. Will raise an error\n",
            " |              if no token is passed and the user is not logged-in.\n",
            " |          branch (Optional :obj:`str`):\n",
            " |              The git branch on which to push the dataset.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
            " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
            " |              (like `\"500MB\"` or `\"1GB\"`).\n",
            " |          shard_size (Optional :obj:`int`):\n",
            " |              Deprecated: 'shard_size' was renamed to 'max_shard_size' in version 2.1.1 and will be removed in 2.4.0.\n",
            " |          embed_external_files (:obj:`bool`, default ``True``):\n",
            " |              Whether to embed file bytes in the shards.\n",
            " |              In particular, this will do the following before the push for the fields of type:\n",
            " |      \n",
            " |              - :class:`Audio` and class:`Image`: remove local path information and embed file content in the Parquet files.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
            " |      ```\n",
            " |  \n",
            " |  remove_columns(self, column_names: Union[str, List[str]]) -> 'DatasetDict'\n",
            " |      Remove one or several column(s) from each split in the dataset\n",
            " |      and the features associated to the column(s).\n",
            " |      \n",
            " |      The transformation is applied to all the splits of the dataset dictionary.\n",
            " |      \n",
            " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
            " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.remove_columns(\"label\")\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text'],\n",
            " |              num_rows: 8530\n",
            " |          })\n",
            " |          validation: Dataset({\n",
            " |              features: ['text'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  rename_column(self, original_column_name: str, new_column_name: str) -> 'DatasetDict'\n",
            " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      You can also rename a column using :func:`Dataset.map` with `remove_columns` but the present method:\n",
            " |          - takes care of moving the original features under the new column name.\n",
            " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
            " |      \n",
            " |      Args:\n",
            " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
            " |          new_column_name (:obj:`str`): New name for the column.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.rename_column(\"label\", \"label_new\")\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text', 'label_new'],\n",
            " |              num_rows: 8530\n",
            " |          })\n",
            " |          validation: Dataset({\n",
            " |              features: ['text', 'label_new'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text', 'label_new'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  rename_columns(self, column_mapping: Dict[str, str]) -> 'DatasetDict'\n",
            " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
            " |      the new column names.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_mapping (:obj:`Dict[str, str]`): A mapping of columns to rename to their new names\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`: A copy of the dataset with renamed columns\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text_new', 'label_new'],\n",
            " |              num_rows: 8530\n",
            " |          })\n",
            " |          validation: Dataset({\n",
            " |              features: ['text_new', 'label_new'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text_new', 'label_new'],\n",
            " |              num_rows: 1066\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  reset_format(self)\n",
            " |      Reset ``__getitem__`` return format to python objects and all columns.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Same as ``self.set_format()``\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
            " |      >>> ds[\"train\"].format\n",
            " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': 'numpy'}\n",
            " |      >>> ds.reset_format()\n",
            " |      >>> ds[\"train\"].format\n",
            " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': None}\n",
            " |      ```\n",
            " |  \n",
            " |  save_to_disk(self, dataset_dict_path: str, fs=None)\n",
            " |      Saves a dataset dict to a filesystem using either :class:`~filesystems.S3FileSystem` or\n",
            " |      ``fsspec.spec.AbstractFileSystem``.\n",
            " |      \n",
            " |      For :class:`Image` and :class:`Audio` data:\n",
            " |      \n",
            " |      If your images and audio files are local files, then the resulting arrow file will store paths to these files.\n",
            " |      If you want to include the bytes or your images or audio files instead, you must `read()` those files first.\n",
            " |      This can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> def read_image_file(example):\n",
            " |      ...     with open(example[\"image\"].filename, \"rb\") as f:\n",
            " |      ...         return {\"image\": {\"bytes\": f.read()}}\n",
            " |      >>> ds = ds.map(read_image_file)\n",
            " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
            " |      ```\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> def read_audio_file(example):\n",
            " |      ...     with open(example[\"audio\"][\"path\"], \"rb\") as f:\n",
            " |      ...         return {\"audio\": {\"bytes\": f.read()}}\n",
            " |      >>> ds = ds.map(read_audio_file)\n",
            " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          dataset_dict_path (``str``): Path (e.g. `dataset/train`) or remote URI\n",
            " |              (e.g. `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset dict will be\n",
            " |              saved to.\n",
            " |          fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, defaults ``None``):\n",
            " |              Instance of the remote filesystem used to download the files from.\n",
            " |  \n",
            " |  set_format(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
            " |      Set ``__getitem__`` return format (type and columns)\n",
            " |      The format is set for every dataset in the dataset dictionary\n",
            " |      \n",
            " |      Args:\n",
            " |          type (:obj:`str`, optional): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n",
            " |              None means ``__getitem__`` returns python objects (default)\n",
            " |          columns (:obj:`List[str]`, optional): columns to format in the output.\n",
            " |              None means ``__getitem__`` returns all columns (default).\n",
            " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
            " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |      \n",
            " |      It is possible to call ``map`` after calling ``set_format``. Since ``map`` may add new columns, then the list of formatted columns\n",
            " |      gets updated. In this case, if you apply ``map`` on a dataset to add a new column, then this column will be formatted:\n",
            " |      \n",
            " |          new formatted columns = (all columns - previously unformatted columns)\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
            " |      >>> ds[\"train\"].format\n",
            " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': 'numpy'}\n",
            " |      ```\n",
            " |  \n",
            " |  set_transform(self, transform: Union[Callable, NoneType], columns: Union[List, NoneType] = None, output_all_columns: bool = False)\n",
            " |      Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n",
            " |      The transform is set for every dataset in the dataset dictionary\n",
            " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n",
            " |      \n",
            " |      Args:\n",
            " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
            " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
            " |              This function is applied right before returning the objects in ``__getitem__``.\n",
            " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
            " |              If specified, then the input batch of the transform only contains those columns.\n",
            " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
            " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
            " |  \n",
            " |  shuffle(self, seeds: Union[int, Dict[str, Union[int, NoneType]], NoneType] = None, seed: Union[int, NoneType] = None, generators: Union[Dict[str, numpy.random._generator.Generator], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_names: Union[Dict[str, Union[str, NoneType]], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000) -> 'DatasetDict'\n",
            " |      Create a new Dataset where the rows are shuffled.\n",
            " |      \n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Currently shuffling uses numpy random generators.\n",
            " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
            " |      \n",
            " |      Args:\n",
            " |          seeds (`Dict[str, int]` or `int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n",
            " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
            " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
            " |              You can provide one :obj:`seed` per dataset in the dataset dictionary.\n",
            " |          seed (Optional `int`): A seed to initialize the default BitGenerator if ``generator=None``. Alias for seeds (a `ValueError` is raised if both are provided).\n",
            " |          generators (Optional `Dict[str, np.random.Generator]`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
            " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
            " |              You have to provide one :obj:`generator` per dataset in the dataset dictionary.\n",
            " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          indices_cache_file_names (`Dict[str, str]`, optional): Provide the name of a path for the cache file. It is used to store the\n",
            " |              indices mappings instead of the automatically generated cache file name.\n",
            " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
            " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds[\"train\"][\"label\"][:10]\n",
            " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " |      \n",
            " |      # set a seed\n",
            " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
            " |      >>> shuffled_ds[\"train\"][\"label\"][:10]\n",
            " |      [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            " |      ```\n",
            " |  \n",
            " |  sort(self, column: str, reverse: bool = False, kind: str = None, null_placement: str = 'last', keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_names: Union[Dict[str, Union[str, NoneType]], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000) -> 'DatasetDict'\n",
            " |      Create a new dataset sorted according to a column.\n",
            " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
            " |      \n",
            " |      Currently sorting according to a column name uses pandas sorting algorithm under the hood.\n",
            " |      The column should thus be a pandas compatible type (in particular not a nested type).\n",
            " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
            " |      \n",
            " |      Args:\n",
            " |          column (:obj:`str`): column name to sort by.\n",
            " |          reverse (:obj:`bool`, default `False`): If True, sort by descending order rather then ascending.\n",
            " |          kind (:obj:`str`, optional): Pandas algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
            " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
            " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
            " |          null_placement (:obj:`str`, default `last`):\n",
            " |              Put `None` values at the beginning if ‘first‘; ‘last‘ puts `None` values at the end.\n",
            " |      \n",
            " |              *New in version 1.14.2*\n",
            " |          keep_in_memory (:obj:`bool`, default `False`): Keep the sorted indices in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the sorted indices\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          indices_cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
            " |              indices mapping instead of the automatically generated cache file name.\n",
            " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
            " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
            " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds[\"train\"][\"label\"][:10]\n",
            " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " |      >>> sorted_ds = ds.sort(\"label\")\n",
            " |      >>> sorted_ds[\"train\"][\"label\"][:10]\n",
            " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " |      ```\n",
            " |  \n",
            " |  unique(self, column: str) -> Dict[str, List]\n",
            " |      Return a list of the unique elements in a column for each split.\n",
            " |      \n",
            " |      This is implemented in the low-level backend and as such, very fast.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (:obj:`str`):\n",
            " |              column name (list all the column names with :func:`datasets.Dataset.column_names`)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Dict[:obj:`str`, :obj:`list`]: Dictionary of unique elements in the given column.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.unique(\"label\")\n",
            " |      {'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n",
            " |      ```\n",
            " |  \n",
            " |  with_format(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs) -> 'DatasetDict'\n",
            " |      Set ``__getitem__`` return format (type and columns). The data formatting is applied on-the-fly.\n",
            " |      The format ``type`` (for example \"numpy\") is used to format batches when using ``__getitem__``.\n",
            " |      The format is set for every dataset in the dataset dictionary\n",
            " |      \n",
            " |      It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.with_transform`.\n",
            " |      \n",
            " |      Contrary to :func:`datasets.DatasetDict.set_format`, ``with_format`` returns a new DatasetDict object with new Dataset objects.\n",
            " |      \n",
            " |      Args:\n",
            " |          type (:obj:`str`, optional):\n",
            " |              Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n",
            " |              None means ``__getitem__`` returns python objects (default)\n",
            " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
            " |              None means ``__getitem__`` returns all columns (default)\n",
            " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
            " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds[\"train\"].format\n",
            " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': None}\n",
            " |      >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
            " |      >>> ds[\"train\"].format\n",
            " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': 'tensorflow'}\n",
            " |      ```\n",
            " |  \n",
            " |  with_transform(self, transform: Union[Callable, NoneType], columns: Union[List, NoneType] = None, output_all_columns: bool = False) -> 'DatasetDict'\n",
            " |      Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n",
            " |      The transform is set for every dataset in the dataset dictionary\n",
            " |      \n",
            " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`.\n",
            " |      \n",
            " |      Contrary to :func:`datasets.DatasetDict.set_transform`, ``with_transform`` returns a new DatasetDict object with new Dataset objects.\n",
            " |      \n",
            " |      Args:\n",
            " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
            " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
            " |              This function is applied right before returning the objects in ``__getitem__``.\n",
            " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
            " |              If specified, then the input batch of the transform only contains those columns.\n",
            " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
            " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> def encode(example):\n",
            " |      ...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
            " |      >>> ds = ds.with_transform(encode)\n",
            " |      >>> ds[\"train\"][0]\n",
            " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            " |       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            " |       1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
            " |       'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n",
            " |              112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n",
            " |              112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n",
            " |              170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n",
            " |              179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n",
            " |              188,  1566,  7912, 14516,  6997,   119,   102]),\n",
            " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            " |              0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  from_csv(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Union[datasets.features.features.Features, NoneType] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
            " |      Create DatasetDict from CSV file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n",
            " |          features (:class:`Features`, optional): Dataset features.\n",
            " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
            " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
            " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :meth:`pandas.read_csv`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import DatasetDict\n",
            " |      >>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n",
            " |      ```\n",
            " |  \n",
            " |  from_json(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Union[datasets.features.features.Features, NoneType] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
            " |      Create DatasetDict from JSON Lines file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (path-like or list of path-like): Path(s) of the JSON Lines file(s).\n",
            " |          features (:class:`Features`, optional): Dataset features.\n",
            " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
            " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
            " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`JsonConfig`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import DatasetDict\n",
            " |      >>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n",
            " |      ```\n",
            " |  \n",
            " |  from_parquet(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Union[datasets.features.features.Features, NoneType] = None, cache_dir: str = None, keep_in_memory: bool = False, columns: Union[List[str], NoneType] = None, **kwargs) -> 'DatasetDict'\n",
            " |      Create DatasetDict from Parquet file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n",
            " |          features (:class:`Features`, optional): Dataset features.\n",
            " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
            " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
            " |          columns (:obj:`List[str]`, optional): If not None, only these columns will be read from the file.\n",
            " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
            " |              'a.b', 'a.c', and 'a.d.e'.\n",
            " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`ParquetConfig`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import DatasetDict\n",
            " |      >>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n",
            " |      ```\n",
            " |  \n",
            " |  from_text(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Union[datasets.features.features.Features, NoneType] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
            " |      Create DatasetDict from text file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (dict of path-like): Path(s) of the text file(s).\n",
            " |          features (:class:`Features`, optional): Dataset features.\n",
            " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
            " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
            " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`TextConfig`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import DatasetDict\n",
            " |      >>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n",
            " |      ```\n",
            " |  \n",
            " |  load_from_disk(dataset_dict_path: str, fs=None, keep_in_memory: Union[bool, NoneType] = None) -> 'DatasetDict'\n",
            " |      Load a dataset that was previously saved using :meth:`save_to_disk` from a filesystem using either\n",
            " |      :class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``.\n",
            " |      \n",
            " |      Args:\n",
            " |          dataset_dict_path (:obj:`str`): Path (e.g. ``\"dataset/train\"``) or remote URI (e.g.\n",
            " |              ``\"s3//my-bucket/dataset/train\"``) of the dataset dict directory where the dataset dict will be loaded\n",
            " |              from.\n",
            " |          fs (:class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n",
            " |              Instance of the remote filesystem used to download the files from.\n",
            " |          keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the\n",
            " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
            " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
            " |              :ref:`load_dataset_enhancing_performance` section.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`DatasetDict`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = load_from_disk('path/to/dataset/directory')\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  cache_files\n",
            " |      The cache files containing the Apache Arrow table backing each split.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.cache_files\n",
            " |      {'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n",
            " |       'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n",
            " |       'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n",
            " |      ```\n",
            " |  \n",
            " |  column_names\n",
            " |      Names of the columns in each split of the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.column_names\n",
            " |      {'test': ['text', 'label'],\n",
            " |       'train': ['text', 'label'],\n",
            " |       'validation': ['text', 'label']}\n",
            " |      ```\n",
            " |  \n",
            " |  data\n",
            " |      The Apache Arrow tables backing each split.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.data\n",
            " |      ```\n",
            " |  \n",
            " |  num_columns\n",
            " |      Number of columns in each split of the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.num_columns\n",
            " |      {'test': 2, 'train': 2, 'validation': 2}\n",
            " |      ```\n",
            " |  \n",
            " |  num_rows\n",
            " |      Number of rows in each split of the dataset (same as :func:`datasets.Dataset.__len__`).\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.num_rows\n",
            " |      {'test': 1066, 'train': 8530, 'validation': 1066}\n",
            " |      ```\n",
            " |  \n",
            " |  shape\n",
            " |      Shape of each split of the dataset (number of columns, number of rows).\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
            " |      >>> ds.shape\n",
            " |      {'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      True if the dictionary has the specified key, else False.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __init__(self, /, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the dict keys.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(...)\n",
            " |      D.__sizeof__() -> size of D in memory, in bytes\n",
            " |  \n",
            " |  clear(...)\n",
            " |      D.clear() -> None.  Remove all items from D.\n",
            " |  \n",
            " |  copy(...)\n",
            " |      D.copy() -> a shallow copy of D\n",
            " |  \n",
            " |  get(self, key, default=None, /)\n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  items(...)\n",
            " |      D.items() -> a set-like object providing a view on D's items\n",
            " |  \n",
            " |  keys(...)\n",
            " |      D.keys() -> a set-like object providing a view on D's keys\n",
            " |  \n",
            " |  pop(...)\n",
            " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
            " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
            " |  \n",
            " |  popitem(self, /)\n",
            " |      Remove and return a (key, value) pair as a 2-tuple.\n",
            " |      \n",
            " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
            " |      Raises KeyError if the dict is empty.\n",
            " |  \n",
            " |  setdefault(self, key, default=None, /)\n",
            " |      Insert key with a value of default if key is not in the dictionary.\n",
            " |      \n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  update(...)\n",
            " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
            " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
            " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
            " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
            " |  \n",
            " |  values(...)\n",
            " |      D.values() -> an object providing a view on D's values\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from builtins.dict:\n",
            " |  \n",
            " |  fromkeys(iterable, value=None, /) from builtins.type\n",
            " |      Create a new dictionary with keys from iterable and values set to value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from builtins.dict:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload data to google drive"
      ],
      "metadata": {
        "id": "MRkjkb_w4M-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stkOeUp0RJso",
        "outputId": "08279b52-9a40-48b4-8975-03b51725f180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6P5CNNIWhfW"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Push to the drive directly\n",
        "common_voice.save_to_disk(\"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice\") ## Set your path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwOWXtThXS7Y",
        "outputId": "9304d7e0-da64-449b-915a-d51a5051f920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'test', 'dataset_dict.json']\n",
            "['state.json', 'dataset_info.json', 'dataset.arrow']\n",
            "['dataset.arrow', 'state.json', 'dataset_info.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/\"))\n",
        "print(os.listdir(\"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/train\"))\n",
        "print(os.listdir(\"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H0rl1mk_gw8",
        "outputId": "bd258aed-f8e4-4ed6-dc9c-f234d9405406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17419676137\n"
          ]
        }
      ],
      "source": [
        "def get_dir_size(path='/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/train'):\n",
        "    total = 0\n",
        "    with os.scandir(path) as it:\n",
        "        for entry in it:\n",
        "            if entry.is_file():\n",
        "                total += entry.stat().st_size\n",
        "            elif entry.is_dir():\n",
        "                total += get_dir_size(entry.path)\n",
        "    return total\n",
        "sz = get_dir_size(path=\"/root/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/\")\n",
        "print(sz)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to reduce the size, we can compress the common_voice folder and store only the zip."
      ],
      "metadata": {
        "id": "_6mlcNX34R3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice.zip -i /content/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/"
      ],
      "metadata": {
        "id": "DBL603HD4cOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Data to Hopsworks"
      ],
      "metadata": {
        "id": "kPK8UWPN4ILb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWUkV38NeAaB"
      },
      "outputs": [],
      "source": [
        "#upload to hopsworks\n",
        "dataset_api = project.get_dataset_api()\n",
        "\n",
        "# create the directories in Hopsworks in the file browser inside some existing dataset\n",
        "# common_voice/train\n",
        "# common_voice/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ffde0b30a79b49d28680aa3176856e4f",
            "ece39493c2bf4f8ab7aeb103b2e29a7a",
            "35a438eca5b84fe69569d2759ec27a16",
            "be65da8d326e4f32b4cab50a3f148e32",
            "d218133b8aa94ecfa37f59ecf735cfd3",
            "c28d8aa0745541688da968d9c2c18c70",
            "7b158b7045cb4a37ac958ef8ae8b621f",
            "9704690bba5a455f8b0f9d598dae09d9",
            "43ae3e3addaa487eb17156ab05109f70",
            "e67a77bfaa08459a809ca37b006fa6bf",
            "40e4daa9ff004cc1bd4f211db9a92de3"
          ]
        },
        "id": "_3dt5UfnX-pN",
        "outputId": "6b718ed4-507e-4921-b6f3-15b62dc05da1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/11871608720 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffde0b30a79b49d28680aa3176856e4f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# upload all of the local files to the common_voice directory you created in Hopsworks\n",
        "\n",
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/train/dataset.arrow\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/train/\", overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/train/dataset_info.json\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/train/\", overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d803b974fc234d2a97edef85564c639a",
            "3c4e380c890d47de92c36b72cdd5444d",
            "275f51994912495eb0fee7d3e971813b",
            "566ade7faf024663b29ba02dae7e01a6",
            "4945644310e649c6ae349eb33882727b",
            "b62342ad260a4052b658dbb76d7301f7",
            "63c39be90dbe40a18c4e17ddeb88d505",
            "34cc1e63b6954419bc1727ef52176c1a",
            "705bb358bec14f3981ee94055387d63b",
            "430f19abae074031852ec77c3665f902",
            "cd932f3862c04111858bbde08ee7ccb9"
          ]
        },
        "id": "5RvbRokrxOGR",
        "outputId": "70edb4da-771f-4784-808b-32206ea1f533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/4909 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d803b974fc234d2a97edef85564c639a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/train/state.json\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/train/\", overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d17bb9a095ee4ed0a38ae65766cedea7",
            "19caf03a706444579772b2eae4fc90b2",
            "fedead187ea24fd69af48e7e54d3508e",
            "273c40b14d8b4f85981123d8129262bf",
            "431300731fdf4a3983a7f1b5b010a5ab",
            "379147010ad441ec8c64e1152e2d9ad6",
            "13c75ecc78454a6fb04cffcec0ee1a0d",
            "dd61155fd4b34062af7a1e5cdc75531c",
            "93e7e75fa45a40d790f5644734a04d42",
            "a90bd6690cef4750b8c904c9d4e3314c",
            "ad73843f45dd4a8d8c79acf8648a83ce"
          ]
        },
        "id": "42Fpl-ilxTVD",
        "outputId": "a729474e-6131-44f9-f11d-7bd6c5aaab98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/267 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17bb9a095ee4ed0a38ae65766cedea7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/test/dataset.arrow\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/test/\", overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac43eac25fe0494f905e4983f9a71b3d",
            "bb5f22929f274c4ea3ddd72cae590534",
            "6fbf90c330834c6da17ed341bcb6b243",
            "d66632156a0e4d3c85892b07f5df9add",
            "d36c449825cc4fcc9f77a2a62b580bd8",
            "c8800ed668f54910b769c952c7fea4b3",
            "0a95bbf9d4184b738687ac3cff61a911",
            "dbfc0f4ec39d4ee0ab7cc6cb4eb5e2cc",
            "d6184b9ec3a445b1843cc239b47f82f9",
            "a2b24b71938d4d0295abae04377fa57c",
            "d58aa547b3d3412bb187528cad6e6fbc"
          ]
        },
        "id": "GCToARaUus70",
        "outputId": "f7cb7215-b1a4-497f-9069-0fac3bd985ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/4868700344 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac43eac25fe0494f905e4983f9a71b3d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/test/dataset_info.json\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/test/\", overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f4f042a8600e4d6db8584cd3c559e1a7",
            "f2c25c6dc2624474875f786a83b17899",
            "fc1d36bfe5a647e6ac654497f1f41338",
            "19ab89442dbb4ca0a0f7a16e06d302d1",
            "41d7f5db6a5a47e2b174cec0ee2b3820",
            "03b8bf3ba3724cac8efc3a606e7c997e",
            "8d1b1e73e514413aa5c04691d153c6c1",
            "8743f1202a1540c1a0abc7842b19d55b",
            "42f1d308f08848fd818ede838cedcb6e",
            "9c33fb645132425aba4a7fa6b2e206de",
            "af6c8f52b52c4a56900a099cdb0a908e"
          ]
        },
        "id": "0FCxpW9-xYZI",
        "outputId": "b997e3be-3528-41bd-ffa0-5617fc537855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/4909 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4f042a8600e4d6db8584cd3c559e1a7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file_path = dataset_api.upload(\n",
        "    local_path = \"/content/drive/MyDrive/KTH/Fall22/P2/ID2223/lab2/common_voice/test/state.json\", \n",
        "    upload_path = \"ID2223_Lab1_ZS_Training_Datasets/common-voice/test/\", overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "58a12ff183ba4e759dd15f007bb6f6e7",
            "c1712fa6ea354c41b57e542d8f101f40",
            "f13183409112403099266a2d6ea8fd99",
            "f3ecff792c764bc9bef735f15ed95d7a",
            "8a2152718bd04c3fb183eb71d43bf661",
            "22f0f36679ef4276a0dde8dcded4143f",
            "b97cebf703b64e6298f58a44759566b4",
            "f36fa55988674721a46a34dda80c9f0e",
            "87b546137d9a4a1980ace4eccc3d9528",
            "9fd02a6d038f4d1b9e60c1c62e0964d9",
            "b23daee7e1cf4fdb906d7a0e17a783d5"
          ]
        },
        "id": "MLccFcv-xiO5",
        "outputId": "baaa50fc-7fad-4865-fa73-ed679f473223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading: 0.000%|          | 0/255 elapsed<00:00 remaining<?"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58a12ff183ba4e759dd15f007bb6f6e7"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}